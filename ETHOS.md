
# Technical definition of the expected system (based on the paper itself):

## Introduction:

The co-scientist is designed to act as a helpful assistant and collaborator to scientists and to help accelerate
the scientific discovery process. The system is a compound, multi-agent AI system [11] building on Gemini
2.0 and designed to mirror the reasoning process underpinning the scientific method [12]. Given a research
goal specified in natural language, the system can search and reason over relevant literature to summarize
and synthesize prior work and build on it to propose novel, original research hypotheses and experimental
protocols for downstream validations (Figure 1a). The co-scientist provides grounding for its recommendations
by citing relevant literature and explaining the reasoning behind its proposals.
This work does not aim to completely automate the scientific process with AI. Instead, the co-scientist is
purpose-built for a “scientist-in-the-loop” collaborative paradigm, to help domain experts augment their
hypothesis generation process and guide the exploration that follows. Scientists can specify their research
goals in simple natural language, including informing the system of desirable attributes for the hypotheses or
research proposals it should create and the constraints that the synthesized outputs should satisfy. They can
also collaborate and provide feedback in a variety of ways, including directly supplying their own ideas and
hypotheses, refining those generated by the system, or using natural language chat to guide the system and
ensure alignment with their expertise.
The co-scientist works through a significant scaling of the test-time compute paradigm [13–15] to iteratively
reason, evolve, and improve the outputs as it gathers more knowledge and understanding. Underpinning
the system are thinking and reasoning steps—notably a self-play based scientific debate step for generating
novel research hypotheses; tournaments that compare and rank hypotheses via the process of finding win and
loss patterns, and a hypothesis evolution process to improve their quality. Finally, the agentic nature of the
system enables it to recursively self-critique its output and use tools such as web-search to provide itself with
feedback to iteratively refine its hypotheses and research proposals.
While the co-scientist system is general-purpose and applicable across multiple scientific disciplines, in this
study we focus our development and validation of the system to biomedicine. We validate the co-scientist’s
capability in three impactful areas of biomedicine with varied complexity: (1) drug repurposing, (2) novel
treatment targets discovery, and (3) new mechanistic explanations for antimicrobial resistance (Figure 1b).

Overall, our key contributions are summarized as follows:
• Introducing an AI co-scientist. We develop and introduce an AI co-scientist that goes beyond literature
summarization and “deep research” tools to assist scientists in uncovering new knowledge, novel hypothesis
generation and experimental planning.
• Significant scaling of test-time compute paradigm for scientific reasoning. The co-scientist
is built on a Gemini 2.0 multi-agent architecture, utilizing an asynchronous task execution framework.
This framework allows the system to flexibly allocate computational resources to scientific reasoning,
mirroring key aspects of the scientific method. Specifically, the system uses self-play strategies, including a
scientific debate and a tournament-based evolution process, to iteratively refine hypotheses and research
proposals creating a self-improving loop. Using automated evaluations across 15 complex expert curated
open scientific goals, we demonstrate the benefits of scaling the test-time compute paradigm with the AI
co-scientist outperforming other state-of-the-art (SOTA) agentic and reasoning models in generating high
quality hypotheses for complex problems.
• Expert-in-the-loop scientific workflow. Our system is designed for collaboration with scientists. The
system can flexibly incorporate conversational feedback in natural language from scientists and co-develop,
evolve and refine outputs.
• End-to-end validation of the co-scientist in important topics in biomedicine. We present
end-to-end validation of novel AI-generated hypotheses through new empirical findings in three distinct
and increasingly complex areas of biomedicine: drug repurposing, novel target discovery, and antimicrobial
resistance. The AI co-scientist predicts novel repurposing drugs for AML, identifies novel epigenetic
treatment targets grounded in preclinical evidence for liver fibrosis, and proposes novel mechanisms for
gene transfer in bacterial evolution and antimicrobial resistance. These discoveries from the AI co-scientist
have been validated in wet-lab settings and are detailed in separate, co-timed technical reports.

## The AI co-scientist system design and experimental validation summary. 
(a) Here, we illustrate the
different components of the the AI co-scientist multi-agent system, and its interaction paradigm with scientists. Given a research goal in natural language, the co-scientist generates novel research hypotheses and proposals. The system employs specialized agents — Generation, Reflection, Ranking, Evolution, Proximity (which evaluates relatedness), Meta-review (which provides high level analysis) — to continuously generate, debate, and evolve research hypotheses within a tournament framework. Feedback from the tournament enables iterative improvement, creating a self-improving loop towards novel and high-quality outputs. The co-scientist leverages tools, including web search and specialized AI models to improve the grounding and quality of generated research hypotheses. Scientists can converse with the co-scientist in natural language to specify research goals, incorporate constraints, provide feedback and suggest new directions for explorations via the designated user interface. (b) We perform end-to-end validation of the co-scientist generated hypotheses in three important topics of biomedicine with varied complexity— suggesting novel drug repurposing candidates for acute myeloid leukemia (AML) (upper panel), discovering novel epigenetic targets for liver fibrosis treatment (middle panel), and recapitulating the discovery of novel mechanism of gene transfer evolution in bacteria key to anti-microbial resistance (lower panel). The co-scientist’s hypotheses for these three settings are externally, independently validated by in vitro laboratory experiments and detailed in separate preprints co-timed with this work. In the figure, blue denotes expert scientist inputs while red denotes the co-scientist agents or outputs.


## Introducing the AI co-scientist
This section describes the technical details, agents, and framework comprising the co-scientist system. The
co-scientist employs a multi-agent architecture built upon Gemini 2.0, integrated within an asynchronous task
execution framework. This framework allows for flexible scaling of test-time compute resources, facilitating
advanced scientific reasoning.
Given a research goal specified by an expert scientist in natural language, the co-scientist generates hypotheses
and research proposals that adhere to the following default criteria:
• Alignment with the provided research goal. The generated outputs must precisely align with the
research goals, preferences and constraints defined by the scientist.
• Plausibility. The system outputs should be free of readily apparent flaws. Any potential contradictions
with prior literature or established knowledge must be explicitly stated and justified.
• Novelty. A key objective of the co-scientist system is to generate novel hypotheses, conjectures, and
research plans grounded in prior literature, rather than simply synthesizing existing information (a
capability already addressed by existing “deep research” tools [62]).
• Testability. The system outputs should be amenable to empirical validation within the constraints
specified by the scientist.
• Safety. The system outputs will be controlled to prevent enabling unsafe, unethical, or harmful research.
Aside from these default criteria, the co-scientist can be configured with additional criteria, preferences, and
constraints as needed. For instance, it can be configured to generate outputs in formats preferred by the
researcher to improve interpretability and readability.
Throughout this section, we employ a recurring example: generating hypotheses for exploring the biological
mechanisms of Amyotrophic Lateral Sclerosis (ALS) to illustrate the various components of the co-scientist
system. While this example has been reviewed by domain experts, it remains illustrative and may contain
errors. Importantly, this example does not aim to suggest potential therapeutic avenues for ALS and should
be interpreted with utmost caution. All the examples are listed in the Appendix Section A.3.

### 3.1 The AI co-scientist system overview
At a high level, the co-scientist system comprises four key components:
• Natural language interface. Scientists interact with and supervise the system primarily through
natural language. This allows them to not only define the initial research goal but also refine it at any
time, provide feedback on generated hypotheses (including their own solutions), and generally guide the
system’s progress.
• Asynchronous task framework. The co-scientist employs a multi-agent system where specialized
agents operate as worker processes within an asynchronous, continuous, and configurable task execution
framework. A dedicated Supervisor agent manages the worker task queue, assigns specialized agents to
these processes, and allocates resources. This design enables the system to flexibly and effectively utilize
computational resources and iteratively improve its scientific reasoning capabilities.
• Specialized agents. Following inductive biases and scientific priors derived from the scientific method,
the process of scientific reasoning and hypothesis generation is broken down into sub-tasks. Individual, specialized agents, each equipped with customized instruction prompts, are designed to execute these
sub-tasks. These agents operate as workers coordinated by the Supervisor agent.
• Context memory. In order to enable iterative computation and scientific reasoning over long time
horizons, the co-scientist uses a persistent context memory to store and retrieve states of the agents and
the system during the course of the computation.
The Gemini 2.0 model is the foundational LLM underpinning all agents in the co-scientist system. The specific
co-scientist design was arrived at with iterative developments and is reflective of the current capabilities of
the underlying LLMs.

### 3.2 From research goal to research plan configuration
The research goal, specified by the scientist, serves as the entry point to the co-scientist system. Leveraging the
multimodal and long context capabilities of Gemini 2.0 models, the co-scientist efficiently processes research
goals of varying complexity, from simple statements to extensive documents spanning tens of thousands of
natural language tokens or other relevant data (e.g., including hundreds of prior publication PDFs). The
research goal may also incorporate specific constraints, attributes, and preferences related to the scientist’s
particular laboratory setting or field of work.
The co-scientist system then parses the goal to derive a research plan configuration for generating research
proposals. This configuration captures the desired proposal preferences, attributes, and constraints. For
example, it specifies whether the co-scientist should exclusively propose novel hypotheses. It also specifies
the criteria for evaluating hypothesis quality, such as novelty and experimental feasibility. These criteria are
then used by the system during its auto-evaluation and improvement phases. The attributes, preferences, and
evaluation criteria can all be customized to a given research goal. To illustrate this process, we present an
example research goal and its corresponding parsed research plan configuration in Appendix Figure A.9, where
the goal is to develop a novel hypothesis related to phosphorylation of the Nuclear Pore Complex (NPC) as a
causative mechanism for ALS [63].
Based on the research plan configuration, the Supervisor agent initiates the creation of a task queue and begins
orchestrating the specialized agents. The system operates continuously and asynchronously. Periodically,
the Supervisor agent calculates a comprehensive set of summary statistics, reflecting the system’s state and
progress toward the specified research goal. These statistics inform decisions regarding resource allocation
and the determination of whether a terminal state for the overall computation has been reached. The state is
periodically written to the associated context memory of the system and leveraged as feedback in subsequent
rounds of computation. It also enables easy restarts in-case of any failure in the system components.

### 3.3 The specialized agents underpinning the AI co-scientist
At the core of the co-scientist system are a coalition of specialized agents, each orchestrated by the Supervisor
agent. These agents are designed to emulate the scientific reasoning process, enabling them to generate novel
hypotheses and research plans. They are also equipped to interact with external tools, such as web search
engines and specialized AI models, through application programming interfaces (APIs). These specialized
agents are enumerated below:
• Generation agent. The agent initiates the research process by generating the initial focus areas,
iteratively extending them and generating a set of initial hypotheses and proposals that address the
research goal. This involves exploring relevant literature using web search, synthesizing existing findings
into novel directions, and engaging in simulated scientific debates for iterative improvement.
• Reflection agent. This agent simulates the role of a scientific peer reviewer, critically examining the
correctness, quality, and novelty of the generated hypotheses and research proposals. Furthermore, it
evaluates the potential of each hypothesis to provide an improved explanation for existing research
observations (identified via literature search and review), particularly those that may be under explained.
• Ranking agent. An important abstraction in the co-scientist system is the notion of a tournament
where different research proposals are evaluated and ranked enabling iterative improvements. The
Ranking agent employs and orchestrates an Elo-based tournament [64] to assess and prioritize the generated hypotheses at any given time. This involves pairwise comparisons, facilitated by simulated
scientific debates, which allow for a nuanced evaluation of the relative merits of each proposal.
• Proximity agent. This agent asynchronously computes a proximity graph for generated hypotheses,
enabling clustering of similar ideas, de-duplication, and efficient exploration of the hypothesis landscape.
• Evolution agent. The co-scientist’s iterative improvement capability relies heavily on this agent, which
continuously refines the top-ranked hypotheses emerging from the tournament. Its refinement strategies
include synthesizing existing ideas, using analogies, leveraging literature for supporting details, exploring
unconventional reasoning, and simplifying concepts for clarity.
• Meta-review agent. This agent also enables the co-scientist’s continuous improvement by synthesizing
insights from all reviews, identifying recurring patterns in tournament debates, and using these findings
to optimize other agents’ performance in subsequent iterations. This also enhances the quality and
relevance of generated hypotheses and reviews in subsequent iterations. The agent also synthesizes
top-ranked hypotheses and reviews into a comprehensive research overview for review by the scientist.

The Supervisor agent’s seamless orchestration of these specialized agents enables the development of valid,
novel, and testable hypotheses and research plans tailored to the input research goal.
In summary, the Generation agent curates an initial list of research hypotheses satisfying a research goal.
These are then reviewed by the Reflection agent and evaluated in a tournament by the Ranking agent. The
Evolution, Proximity, and Meta-review agents operate on the tournament state to help improve the quality of
the system outputs.
The Supervisor agent periodically computes and writes to the context memory, a comprehensive suite of
statistics, including the number of hypotheses generated and requiring review, and the progress of the
tournament. These statistics also include analyses of the effectiveness of different hypothesis generation
methodologies (e.g., generating new ideas via the Generation agent vs. improving existing ideas via the
Evolution agent). Based on these statistics, the Supervisor agent then orchestrates subsequent system
operations, i.e., generating new hypotheses, reviews, tournaments, and improvements to existing hypotheses,
by strategically weighting and sampling the specialized agents for execution via the worker processes.
Importantly, the Meta-review agent enables feedback propagation and learning without back-propagation
techniques (e.g., fine-tuning or reinforcement learning) [65]. The Meta-review agent generates feedback applicable to all agents, which is simply appended to their prompts in the next iteration—a capability
facilitated by the long-context search and reasoning capabilities of the underlying Gemini 2.0 models. Through
this feedback loop, the co-scientist continuously learns and improves in subsequent iterations with more
compute scaling.
Finally, while our work leverages Gemini 2.0, the co-scientist framework is model-agnostic and portable to
other similar models or combinations thereof. Future LLM improvements will likely enhance the co-scientist’s
capabilities. The multi-agent architecture of the co-scientist is depicted and summarized in Figure 2.
We now describe the mechanisms of action of the specialized agents in more detail.

#### 3.3.1 Generation agent
The co-scientist Generation agent employs a diverse array of techniques and tools to generate novel hypotheses,
such as the following:
• Literature exploration via web search. The agent iteratively searches the web, retrieves and reads
relevant research articles, and grounds its reasoning by summarizing prior work. It then builds on this
summary to generate novel hypotheses and research plans. An example prompt is given in Appendix
Figure A.1.
• Simulated scientific debates. Here, the Generation agent simulates scientific debates among experts
by employing self-critique and self-play techniques. These debates typically involve multiple turns of
conversations leading to a refined hypothesis generated at the end. An example prompt is given in
Appendix Figure A.2.
• Iterative assumptions identification. The agent iteratively identifies testable intermediate assumptions, which, if proven true, can lead to novel scientific discovery. These plausible assumptions and their
sub-assumptions are identified through conditional reasoning hops and subsequently aggregated into
complete hypotheses.
• Research expansion. To identify previously unexplored areas of the hypothesis space, the Generation
agent reviews existing hypotheses and the research overview and feedback provided by the Meta-review
agent in the previous iteration. This is used to inform additional exploration directions in the research
hypothesis space.
An example hypothesis and research proposal output from the Generation agent is presented in Appendix
Figure A.10 for the aforementioned research goal regarding explaining a basic mechanism related to ALS. The
Generation agent also summarizes and categorizes each generated hypothesis, allowing scientists to quickly
grasp the core ideas.

#### 3.3.2 Reflection agent
Reviews are integral to the co-scientist’s effectiveness in generating novel proposals. The Reflection agent
searches relevant prior work (via web search or a dedicated scientist-provided repository), assesses existing
experimental evidence for or against a given hypothesis, and rigorously verifies the novelty, correctness, and
quality of generated outputs. Effective reviews filter inaccurate and, when stipulated, non-novel hypotheses.
Moreover, they also provide feedback to all other agents, driving continuous improvement. The Reflection
agent employs the following types of review:
• Initial review. Building on the co-scientist’s default evaluation criteria, the Reflection agent performs
an initial review assessing the correctness, quality, novelty, and a preliminary assessment of safety (ethics)
of the generated hypotheses. For a more in-depth discussion on safety considerations see Section 6. This
initial review, which doesn’t use external tools like web search, aims to quickly discard flawed, non-novel,
or otherwise unsuitable hypotheses.
• Full review. If a hypothesis passes the initial review, the Reflection agent performs a full review,
leveraging external tools and web searches to identify relevant articles for improved reasoning and
grounding. This review evaluates the hypothesis’s correctness, quality, and novelty similar to the initial
review but with full literature search. For correctness and quality, the agent scrutinizes underlying assumptions and reasoning. For novelty, it summarizes known aspects of the hypothesis and then
judges their novelty based on existing literature. An example full novelty review is shown in Appendix
Figure A.11, and an example of review critiques is in Appendix Figure A.12. A complete full review
example is shown in Appendix Figure A.13.
• Deep verification review. The Reflection agent also conducts a deep verification review, decomposing
the hypothesis into constituent assumptions. Each assumption is further broken down into fundamental
sub-assumptions, decontextualized, and independently evaluated for correctness to identify invalidating
elements for subsequent filtering. Concurrently, the reasons for potential hypothesis invalidation due
to incorrect assumptions are summarized. This deep verification helps the co-scientist detect subtle
errors within complex hypotheses, such as flaws in reasoning or inaccurate experimental protocols. An
identified error doesn’t necessarily invalidate the core hypothesis; the Reflection agent assesses whether
the incorrect assumption is fundamental to the hypothesis and incorporates this reasoning into the
review. Non-fundamental errors can be addressed during subsequent refinement stages. An example deep
verification review is provided in Appendix Figure A.14 for the previously introduced ALS hypothesis.
We also show another example of a deep verification review via probing questions in the context of drug
repurposing for AML in Appendix Figure A.15.
• Observation review. In addition, the Reflection agent also explores whether a given hypothesis can
account for long-tail observations from prior experimental results. This review aims to determine if
the hypothesis can provide insights on existing experimental findings and observed phenomena within
relevant articles. For each observation, the agent assesses if the hypothesis is a superior explanation
over existing ones, assuming its validity. Positive observations are summarized and appended to the
hypothesis. Note that this review often completes without any important findings (as in the case of
the ALS hypothesis example). An example prompt to generate observations is provided in Appendix
Figure A.3. An illustrative example of an observation review is provided in Appendix Figure A.16 in
the context of an alternate hypothesis for explaining a mechanism of anti-microbial resistance.
• Simulation review. The Reflection agent also reviews hypotheses by simulating them in a step-wise
fashion (e.g., simulating the mechanism of action or the proposed experiment in the proposal). This
simulation allows the agent to identify and summarize potential failure scenarios. This review method is
built on the assumption that frontier LLMs may have developed an internal world model that enables
them to simulate and accurately predict various scientific phenomena.
• Recurrent/tournament review. The Reflection agent adapts its full reviews based on the co-scientist’s
growing knowledge. By analyzing reviewed hypotheses and results of the tournament conducted by the
Ranking agent, the Reflection agent identifies recurring issues and improvement opportunities, refining
its reviews accordingly.
Additionally, the co-scientist can incorporate reviews from expert scientists to guide ranking and improvements
(further discussed in Section 3.4). We aim to have the Reflection agent’s comprehensive set of reviews cover
the common methods scientists employ when critiquing and refining research hypotheses and proposals.

#### 3.3.3 Ranking agent
The AI co-scientist explores numerous hypotheses and research proposals towards a research goal, necessitating
a ranking mechanism to prioritize computational resources toward the most promising candidates. This task
is performed by the Ranking agent. The agent uses an Elo-based tournament [64] to automatically evaluate
and rank all hypotheses, providing supporting rationale. This ranking serves to communicate to scientists
an ordered list of research hypotheses and proposals aligned with the research goal. Despite its assumptions
and limitations [66], Elo remains a good proxy for relative ranking, and it has previously been applied to
rank extracted patterns and ideas in games [67]. In the future, extensions may be considered [68]. We set the
initial Elo rating of 1200 for the newly added hypothesis.
Because the tournament is computationally intensive, the Ranking agent employs several optimization
strategies. Top-ranked hypotheses are compared pairwise in tournament matches through multi-turn scientific
debates [69]. This mitigates ordering bias and focuses on novelty, correctness, and testability. Lower-ranked
hypotheses undergo single-turn comparisons in a pairwise fashion in their tournament match. The agent
concludes each comparison with a decision regarding which hypothesis is better. Appendix Figure A.4 and 

Appendix Figure A.5 show example prompts. Appendix Figure A.17 shows an example of the Ranking agent
conducting a scientific debate match in a tournament to compare two hypotheses.
The Ranking agent prioritizes tournament matches as follows: (1) hypotheses are more likely to be compared
with similar ones (based on the Proximity agent’s graph, described in the next section); (2) newer and
top-ranking hypotheses are prioritized for participation in tournament matches. Successful hypotheses quickly
achieve favorable rankings and this informs the tournament state for subsequent iterations.

#### 3.3.4 Proximity agent
The Proximity agent calculates the similarity between research hypotheses and proposals, and builds a
proximity graph, taking into account the specific research goal. Although it doesn’t directly participate in
hypothesis generation, the Proximity agent assists the Ranking agent in organizing tournament matches and
showcasing a diverse range of ideas related to the research goal. This allows scientists to quickly explore areas
of interest and easily identify related concepts.

#### 3.3.5 Evolution agent
The Evolution agent continuously refines and improves existing hypotheses and proposals using several
approaches including:
• Enhancement through grounding. Here the agent attempts to improve hypotheses by identifying
weaknesses, generating search queries, retrieving and reading articles, suggesting improvements and
elaborating on details to fill reasoning gaps.
• Coherence, practicality and feasibility improvements. The agent aims to address issues and
creates more coherent hypotheses, potentially rectifying underlying problems with invalid initial assumptions. The agent also refines the hypotheses to make them more practical and feasible. Appendix
Figure A.6 provides an example of the feasibility improvement prompt.
• Inspiration from existing hypotheses. The agent additionally creates new hypotheses inspired by
single or multiple top-ranked hypotheses.
• Combination. The agent also attempts to directly combine the best aspects of several top-ranking
hypotheses to create new hypotheses.
• Simplification. The agent simplifies hypotheses for easier verification and testing.
• Out-of-box thinking. The agent also explores out-of-the-box ideas by moving away from a subset of
hypotheses and generating divergent ones. Appendix Figure A.7 provides an example prompt for this.
The Evolution agent generates new hypotheses; it doesn’t modify or replace existing ones. This strategy
protects the quality of top-ranked hypotheses from flawed improvements, as each new hypothesis must also
compete in the tournament. The evolution of research hypotheses and proposals also allows the co-scientist to
iteratively combine different improvement techniques and gradually improve the quality of the results.

#### 3.3.6 Meta-review agent
The Meta-review agent plays a crucial role in the co-scientist’s feedback loop, enabling self-improvement in
scientific reasoning. This agent operates on the tournament state and summarizes common patterns identified
in reviews and scientific debates in the tournament matches into a meta-review critique.
By synthesizing insights from all reviews, the meta-review provides valuable feedback to the Reflection agent,
leading to more thorough and reliable future reviews. This helps prevent oversight of critical details. Consider
the illustrative example of a identifying a repurposing drug candidate for ALS as a research goal: while only
90% of individual reviews might correctly identify a blood-brain barrier permeability issue in a proposed
candidate, the meta-review ensures that all future reviews by the Reflection Agent definitively address this
crucial factor. Hypothesis and research proposal generation is also enhanced by the meta-review’s identification
of recurring issues. While the Generation agent uses this feedback selectively to avoid over fitting to these
review critiques, it helps prevent the recurrence of common issues.
Appendix Figure A.8 provides an example prompt for the meta-review. In Appendix Figure A.18-A.19, we
showcase an example of the summarized meta-review critique generated for the reviews of the previously introduced ALS mechanism hypotheses.
Research overview generation. The Meta-review agent periodically synthesizes top-ranked hypotheses into
a research overview, providing a roadmap for future research. This overview outlines potential research areas
and directions relevant to the research goal, justifying their importance and suggesting specific experiments
within each. Each area includes illustrative example topics. The research overview also serves as an additional
input to the Generation agent in subsequent iterations.
The research overview serves to effectively map the boundary of current knowledge relevant to the research
goal in the co-scientist system and helps highlight future areas of exploration. In Appendix Figure A.20-A.21,
we show an example of a research overview for the ALS mechanism research goal.
The Meta-review agent can further format these overviews using constrained decoding techniques [70] to
adhere to common research publication and grant formats (e.g., National Institute of Health (NIH) Specific
Aims Page format). We demonstrate the effectiveness of this in subsequent sections.
Research contacts identification. The Meta-review agent uses prior literature review to suggest qualified
domain experts for research hypotheses and proposal review, including the reasoning behind each suggestion.
These potential contacts are summarized in the research overview, providing researchers with additional
perspectives and potential avenues for collaborations. An example research contact (with the researcher name
redacted) is shown in Appendix Figure A.22.

### 3.4 Expert-in-the-loop interactions with the co-scientist
The AI co-scientist empowers scientists to actively guide the system through an expert-in-the-loop design
(Figure 2). Scientists can interact with the system in several ways:
• Refine the initial research goal in light of the generated hypotheses and research overview.
• Provide manual reviews of generated hypotheses (see Section 3.3.2 for other system generated review
types), which the co-scientist uses to evaluate and improve the hypotheses and proposals.
• Contribute their own hypotheses and proposals for inclusion in the tournament, where they are ranked
alongside and can be combined with system-generated hypotheses and proposals.
• Direct the co-scientist to follow up on specific research directions (for example restricted to a smaller
collection of prior publications). When this research is referenced in the research goal, the co-scientist
can prioritize generation methods that can access and synthesize it.

### 3.5 Tool use in AI co-scientist
The co-scientist leverages various tools during the generation, review, and improvement of hypotheses and
research proposals. Web search and retrieval are primary tools, important for grounded, up-to-date hypotheses.
For research goals that explore a constrained space of possibilities (e.g., all known cell receptors of a specific
type or all FDA-approved drugs), the co-scientist agents utilize domain-specific tools, such as open databases,
to constrain searches and generate hypotheses. The co-scientist can also index and search a private repository
of publications specified by the scientist.
Finally, the system can utilize and incorporate feedback from specialized AI models like AlphaFold. We
demonstrate this qualitatively with a protein design example in the Appendix Section A.6.

## The AI co-scientist multi-agent architecture design. The co-scientist accepts a natural language research goal
from the user and parses this into a research plan configuration. This plan is then dispatched to the Supervisor agent which evaluates this plan to assigns weights and resources to each specialized agent and subsequently queues them as worker processes in a task queue according to these weights. The worker processes execute the queue of agent actions, and the system ultimately aggregates all information to formulate a research overview with detailed hypotheses and proposals for the scientist. The red boxes in the “The AI co-scientist specialized agents” section denote individual agents each with their own unique logic and role.
The blue boxes indicate the scientist-in-the-loop inputs and feedback. The dark gray arrows represent the information flow
through the co-scientist system, while the red arrows represent the information feedback loop between the specialized agents.


## Example: The AI co-scientist uncovers novel therapeutic targets for liver fibrosis

Liver fibrosis is a severe disease that can progress to liver failure and hepatocellular carcinoma, which has
few treatment options due to the limitations of available animal and in vitro models. However, a recently
developed method for producing human hepatic organoids coupled with a live cell imaging system for liver
fibrosis provides a new avenue for identification of new treatments for liver fibrosis [82–84]. The AI co-scientist
was asked to produce experimentally testable hypotheses concerning the role of epigenetic alterations in liver
fibrosis (“A Novel Hypothesis Regarding Myofibroblast Generation in Liver Fibrosis”); and to identify drugs
targeting epigenetic modifiers that could be used for treatment of liver fibrosis.

The experts selected three (from fifteen) top-ranked co-scientist generated research hypotheses with a
comprehensive research proposal (i.e., experimental design, evaluation methodology, and anticipated results)
for exploring the role of epigenetic modifications in liver fibrosis. The co-scientist identified three novel
epigenetic modifiers with supporting preclinical evidence that could be targeted by existing agents and provide
new treatments for liver fibrosis. Drugs targeting two of the three epigenetic modifiers exhibited significant
anti-fibrotic activity in hepatic organoids without causing cellular toxicity (Figure 12). Since one of them is
an FDA-approved drug for another indication, this creates an opportunity to re-purpose a drug for treatment
of liver fibrosis. These results will be detailed in an upcoming technical report.



## A.2 Prompts for the specialized agents in the AI co-scientist system

### A.2.1 Prompts for the Generation agent

#### Prompt for hypothesis generation after literature review
You are an expert tasked with formulating a novel and robust hypothesis to address
the following objective.
Describe the proposed hypothesis in detail, including specific entities, mechanisms,
and anticipated outcomes.
This description is intended for an audience of domain experts.
You have conducted a thorough review of relevant literature and developed a logical framework
for addressing the objective. The articles consulted, along with your analytical reasoning,
are provided below.
Goal: {goal}
Criteria for a strong hypothesis:
{preferences}
Existing hypothesis (if applicable):
{source_hypothesis}
{instructions}
Literature review and analytical rationale (chronologically ordered, beginning
with the most recent analysis):
{articles_with_reasoning}
Proposed hypothesis (detailed description for domain experts):


#### Prompt for hypothesis generation after scientific debate
You are an expert participating in a collaborative discourse concerning the generation
of a {idea_attributes} hypothesis. You will engage in a simulated discussion with other experts.
The overarching objective of this discourse is to collaboratively develop a novel
and robust {idea_attributes} hypothesis.
Goal: {goal}
Criteria for a high-quality hypothesis:
{preferences}
Instructions:
{instructions}
Review Overview:
{reviews_overview}
Procedure:
Initial contribution (if initiating the discussion):
Propose three distinct {idea_attributes} hypotheses.
Subsequent contributions (continuing the discussion):
* Pose clarifying questions if ambiguities or uncertainties arise.
* Critically evaluate the hypotheses proposed thus far, addressing the following aspects:
- Adherence to {idea_attributes} criteria.
- Utility and practicality.
- Level of detail and specificity.
* Identify any weaknesses or potential limitations.
* Propose concrete improvements and refinements to address identified weaknesses.
* Conclude your response with a refined iteration of the hypothesis.
General guidelines:
* Exhibit boldness and creativity in your contributions.
* Maintain a helpful and collaborative approach.
* Prioritize the generation of a high-quality {idea_attributes} hypothesis.
Termination condition:
When sufficient discussion has transpired (typically 3-5 conversational turns,
with a maximum of 10 turns) and all relevant questions and points have been
thoroughly addressed and clarified, conclude the process by writing "HYPOTHESIS"
(in all capital letters) followed by a concise and self-contained exposition of the finalized idea.
#BEGIN TRANSCRIPT#
{transcript}
#END TRANSCRIPT#
Your Turn:


### Prompt for the Reflection agent

#### Prompt for generating observations which can be explained by the hypothesis
You are an expert in scientific hypothesis evaluation. Your task is to analyze the
relationship between a provided hypothesis and observations from a scientific article.
Specifically, determine if the hypothesis provides a novel causal explanation
for the observations, or if they contradict it.
Instructions:
1. Observation extraction: list relevant observations from the article.
2. Causal analysis (individual): for each observation:
a. State if its cause is already established.
b. Assess if the hypothesis could be a causal factor (hypothesis => observation).
c. Start with: "would we see this observation if the hypothesis was true:".
d. Explain if it’s a novel explanation. If not, or if a better explanation exists,
state: "not a missing piece."
3. Causal analysis (summary): determine if the hypothesis offers a novel explanation
for a subset of observations. Include reasoning. Start with: "would we see some of
the observations if the hypothesis was true:".
4. Disproof analysis: determine if any observations contradict the hypothesis.
Start with: "does some observations disprove the hypothesis:".
5. Conclusion: state: "hypothesis: <already explained, other explanations more likely,
missing piece, neutral, or disproved>".
Scoring:
* Already explained: hypothesis consistent, but causes are known. No novel explanation.
* Other explanations more likely: hypothesis *could* explain, but better explanations exist.
* Missing piece: hypothesis offers a novel, plausible explanation.
* Neutral: hypothesis neither explains nor is contradicted.
* Disproved: observations contradict the hypothesis.
Important: if observations are expected regardless of the hypothesis, and don’t disprove it,
it’s neutral.
Article:
{article}
Hypothesis:
{hypothesis}
Response {provide reasoning. end with: "hypothesis: <already explained, other explanations
more likely, missing piece, neutral, or disproved>".)


### Prompts for the Ranking agent

#### Prompt for hypothesis comparison during tournament
You are an expert evaluator tasked with comparing two hypotheses.
Evaluate the two provided hypotheses (hypothesis 1 and hypothesis 2) and determine which one
is superior based on the specified {idea_attributes}.
Provide a concise rationale for your selection, concluding with the phrase "better idea: <1 or 2>".
Goal: {goal}
Evaluation criteria:
{preferences}
Considerations:
{notes}
Each hypothesis includes an independent review. These reviews may contain numerical scores.
Disregard these scores in your comparative analysis, as they may not be directly comparable across reviews.
Hypothesis 1:
{hypothesis 1}
Hypothesis 2:
{hypothesis 2}
Review of hypothesis 1:
{review 1}
Review of hypothesis 2:
{review 2}
Reasoning and conclusion (end with "better hypothesis: <1 or 2>"):


#### Prompt for hypothesis comparison via simulated scientific debate during tournament
You are an expert in comparative analysis, simulating a panel of domain experts
engaged in a structured discussion to evaluate two competing hypotheses.
The objective is to rigorously determine which hypothesis is superior based on
a predefined set of attributes and criteria.
The experts possess no pre-existing biases toward either hypothesis and are solely
focused on identifying the optimal choice, given that only one can be implemented.
Goal: {goal}
Criteria for hypothesis superiority:
{preferences}
Hypothesis 1:
{hypothesis 1}
Hypothesis 2:
{hypothesis 2}
Initial review of hypothesis 1:
{review1}
Initial review of hypothesis 2:
{review 2}
Debate procedure:
The discussion will unfold in a series of turns, typically ranging from 3 to 5, with a maximum of 10.
Turn 1: begin with a concise summary of both hypotheses and their respective initial reviews.
Subsequent turns:
* Pose clarifying questions to address any ambiguities or uncertainties.
* Critically evaluate each hypothesis in relation to the stated Goal and Criteria.
This evaluation should consider aspects such as:
- Potential for correctness/validity.
- Utility and practical applicability.
- Sufficiency of detail and specificity.
- Novelty and originality.
- Desirability for implementation.
* Identify and articulate any weaknesses, limitations, or potential flaws in either hypothesis.
Additional notes:
{notes}
Termination and judgment:
Once the discussion has reached a point of sufficient depth (typically 3-5 turns, up to 10 turns)
and all relevant questions and concerns have been thoroughly addressed, provide a conclusive judgment.
This judgment should succinctly state the rationale for the selection.
Then, indicate the superior hypothesis by writing the phrase "better idea: ",
followed by "1" (for hypothesis 1) or "2" (for hypothesis 2).


### Prompts for the Evolution agent

#### Prompt for hypothesis feasibility improvement
You are an expert in scientific research and technological feasibility analysis.
Your task is to refine the provided conceptual idea, enhancing its practical implementability
by leveraging contemporary technological capabilities. Ensure the revised concept retains
its novelty, logical coherence, and specific articulation.
Goal: {goal}
Guidelines:
1. Begin with an introductory overview of the relevant scientific domain.
2. Provide a concise synopsis of recent pertinent research findings and related investigations,
highlighting successful methodologies and established precedents.
3. Articulate a reasoned argument for how current technological advancements can facilitate
the realization of the proposed concept.
4. CORE CONTRIBUTION: Develop a detailed, innovative, and technologically viable alternative
to achieve the objective, emphasizing simplicity and practicality.
Evaluation Criteria:
{preferences}
Original Conceptualization:
{hypothesis}
Response:


#### Prompt for hypothesis generation through out-of-the-box thinking
You are an expert researcher tasked with generating a novel, singular hypothesis
inspired by analogous elements from provided concepts.
Goal: {goal}
Instructions:
1. Provide a concise introduction to the relevant scientific domain.
2. Summarize recent findings and pertinent research, highlighting successful approaches.
3. Identify promising avenues for exploration that may yield innovative hypotheses.
4. CORE HYPOTHESIS: Develop a detailed, original, and specific single hypothesis
for achieving the stated goal, leveraging analogous principles from the provided
ideas. This should not be a mere aggregation of existing methods or entities. Think out-of-the-box.
Criteria for a robust hypothesis:
{preferences}
Inspiration may be drawn from the following concepts (utilize analogy and inspiration,
not direct replication):
{hypotheses}
Response:


### Prompt for the Meta-review agent

#### Prompt for meta-review generation
You are an expert in scientific research and meta-analysis.
Synthesize a comprehensive meta-review of provided reviews
pertaining to the following research goal:
Goal: {goal}
Preferences:
{preferences}
Additional instructions:
{instructions}
Provided reviews for meta-analysis:
{reviews}
Instructions:
* Generate a structured meta-analysis report of the provided reviews.
* Focus on identifying recurring critique points and common issues raised by reviewers.
* The generated meta-analysis should provide actionable insights for researchers
developing future proposals.
* Refrain from evaluating individual proposals or reviews;
focus on producing a synthesized meta-analysis.
Response:


## Other resources

In mini_raul/example_desired_system_outputs.md there are system output examples for the system's final development state. 

The system will be developed in an interative way, starting with the basics, so we don't expect the system to produce this quality answers from the beginning. 

This is a goal and can be helpful to compare and build the system in such a way that replicates something similar to these examples.




---
---







# My basic summary of what the system should do:

El sistema tiene el concepto de "session" o "project". Esto es una investigacion puntual sobre algun tema que quiero hacer investigacion. Tienen un ID particular que puede ser un nombre que el usuario determina al principio.
Una session comienza con un user input inicial, que puede tener un objetivo, explicacion del contexto, algunas recomendaciones e informacion para que lea. Puede apuntar hacia distintas fuentes de informacion: links a websites, pdfs, archivos de otros tipos como word, excel, csv, etc.
Entonces el user input contiene textos, links y archivos. Los archivos deberian ser locales, asi que podriamos apuntar a una ruta local para que el sistema lo vaya a buscar.

Despues el proceso sigue de la manera indicada en el paper, donde hay multiples agentes y hay multiples generadores de hipotesis. 
Para generar una hipotesis, cada agente encargado tiene a disposicion tools: web search, buscador de papers, pdf readers and parsers y code execution en python. El sistema puede iterativamente usar tools, recopilar los datos, descargar pdfs, parsearlos y extraer la informacion, ejecutar codigo para abrir archivos y hacer analisis de datos, etc.
Luego, en algun momento, puede cortar ese proceso iterativo donde busca informacion y mezclar, sintetizar y usar toda la informacion recopilada, tanto de su recopilacion como del user input.
Con eso listo, debe generar una hipotesis bien detallada e innovadora para resolver el problema planteado.
La informacion usada debe ser guardada de alguna manera en un knowledge base. Puede ser con KG, databases comunes, vector dbs, en textos, etc. Eso hay que verlo bien pero hay que decidirlo. Debe haber metadata indicando que papers ya fueron leidos y procesados, etc, para luego no hacerlo multiples veces lo mismo (a menos que se considere que sea necesario).

Luego, el proceso debe continuar como indica el paper, con las evaluaciones, torneos, etc... con los multiples agentes y el flow indicado.

Luego, una vez que tenemos un conjunto de hipotesis con sus rankings y puntuaciones, se termina la iteracion y se pasa a escuchar el feedback del usuario. Esta es una etapa obligatoria. Dentro de las hipotesis, se pueden recomendar hacer experimentos especificos, para que el usuario pueda realizarlos y despues entregar los resultados para validar o rechazar hipotesis en conjunto. Las hipotesis tambien deben ser guardadas en la base de conocimiento.
El usuario da feedback de la misma manera que el input inicial, con texto y documentos o links. Aqui pueden haber tambien archivos con data experimental para tener en cuenta.

El proceso vuelve a seguir de la misma manera, con otra iteracion y asi se van construyendo y mejorando las hipotesis y guardando la informacion pertinente. Los agentes de la iteracion 2 ya tienen que tener a disposicion la informacion recopilada, extraida y procesada de la iteracion 1, y la metadata correspondiente, por lo que puede ayudar a profundizar mas al buscar alternativas o fuentes complementarias para reforzas las hipotesis.
Para cada iteracion, los agentes que generan las hipotesis no es que deben continuar con cada una de las hipotesis creadas anteriormente sino que tienen que tener en cuenta las mejores hipotesis de la iteracion anterior mas el feedback del usuario. Y ahi empiezan desde cero pero teniendo en cuenta esas cosas mas toda la info ya almacenada en la base de conocimiento.

Cada session puede cerrarse cuando queramos y despues retomarla. Pueden existir multiples session al mismo tiempo, y por ahora no deberian compartir su base de conocimiento (quizas se pueda implementar posteriormente).
Cada session tiene que tener distintos estados "state". Aqui se determina en que etapa del proyecto se esta, por ejemplo "waiting for user feedback".
Tiene que quedar un registro de todas las hipotesis, que agentes la realizo, como fue evolucionando, etc.

Todo esto es estimativo y puede mejorarse a traves de tus recomendaciones, basandote en las buenas practicas.