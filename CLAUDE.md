You are tasked with designing and building an AI co-scientist system to assist human scientists in generating, refining, and testing research hypotheses. This system should serve as a collaborative partner, enhancing scientists' ability to explore complex scientific problems efficiently. It must be flexible, adaptable across various scientific domains (e.g., biology, chemistry, physics), and capable of integrating with external tools and data sources. The system will interact with scientists through a natural language interface, providing hypotheses, experiment designs, and insights while learning from feedback and experimental results.

Below is the technical definition of the system (based on the paper itself):

Introduction:

The co-scientist is designed to act as a helpful assistant and collaborator to scientists and to help accelerate
the scientific discovery process. The system is a compound, multi-agent AI system [11] building on Gemini
2.0 and designed to mirror the reasoning process underpinning the scientific method [12]. Given a research
goal specified in natural language, the system can search and reason over relevant literature to summarize
and synthesize prior work and build on it to propose novel, original research hypotheses and experimental
protocols for downstream validations (Figure 1a). The co-scientist provides grounding for its recommendations
by citing relevant literature and explaining the reasoning behind its proposals.
This work does not aim to completely automate the scientific process with AI. Instead, the co-scientist is
purpose-built for a “scientist-in-the-loop” collaborative paradigm, to help domain experts augment their
hypothesis generation process and guide the exploration that follows. Scientists can specify their research
goals in simple natural language, including informing the system of desirable attributes for the hypotheses or
research proposals it should create and the constraints that the synthesized outputs should satisfy. They can
also collaborate and provide feedback in a variety of ways, including directly supplying their own ideas and
hypotheses, refining those generated by the system, or using natural language chat to guide the system and
ensure alignment with their expertise.
The co-scientist works through a significant scaling of the test-time compute paradigm [13–15] to iteratively
reason, evolve, and improve the outputs as it gathers more knowledge and understanding. Underpinning
the system are thinking and reasoning steps—notably a self-play based scientific debate step for generating
novel research hypotheses; tournaments that compare and rank hypotheses via the process of finding win and
loss patterns, and a hypothesis evolution process to improve their quality. Finally, the agentic nature of the
system enables it to recursively self-critique its output and use tools such as web-search to provide itself with
feedback to iteratively refine its hypotheses and research proposals.
While the co-scientist system is general-purpose and applicable across multiple scientific disciplines, in this
study we focus our development and validation of the system to biomedicine. We validate the co-scientist’s
capability in three impactful areas of biomedicine with varied complexity: (1) drug repurposing, (2) novel
treatment targets discovery, and (3) new mechanistic explanations for antimicrobial resistance (Figure 1b).

Overall, our key contributions are summarized as follows:
• Introducing an AI co-scientist. We develop and introduce an AI co-scientist that goes beyond literature
summarization and “deep research” tools to assist scientists in uncovering new knowledge, novel hypothesis
generation and experimental planning.
• Significant scaling of test-time compute paradigm for scientific reasoning. The co-scientist
is built on a Gemini 2.0 multi-agent architecture, utilizing an asynchronous task execution framework.
This framework allows the system to flexibly allocate computational resources to scientific reasoning,
mirroring key aspects of the scientific method. Specifically, the system uses self-play strategies, including a
scientific debate and a tournament-based evolution process, to iteratively refine hypotheses and research
proposals creating a self-improving loop. Using automated evaluations across 15 complex expert curated
open scientific goals, we demonstrate the benefits of scaling the test-time compute paradigm with the AI
co-scientist outperforming other state-of-the-art (SOTA) agentic and reasoning models in generating high
quality hypotheses for complex problems.
• Expert-in-the-loop scientific workflow. Our system is designed for collaboration with scientists. The
system can flexibly incorporate conversational feedback in natural language from scientists and co-develop,
evolve and refine outputs.
• End-to-end validation of the co-scientist in important topics in biomedicine. We present
end-to-end validation of novel AI-generated hypotheses through new empirical findings in three distinct
and increasingly complex areas of biomedicine: drug repurposing, novel target discovery, and antimicrobial
resistance. The AI co-scientist predicts novel repurposing drugs for AML, identifies novel epigenetic
treatment targets grounded in preclinical evidence for liver fibrosis, and proposes novel mechanisms for
gene transfer in bacterial evolution and antimicrobial resistance. These discoveries from the AI co-scientist
have been validated in wet-lab settings and are detailed in separate, co-timed technical reports.

Introducing the AI co-scientist
This section describes the technical details, agents, and framework comprising the co-scientist system. The
co-scientist employs a multi-agent architecture built upon Gemini 2.0, integrated within an asynchronous task
execution framework. This framework allows for flexible scaling of test-time compute resources, facilitating
advanced scientific reasoning.
Given a research goal specified by an expert scientist in natural language, the co-scientist generates hypotheses
and research proposals that adhere to the following default criteria:
• Alignment with the provided research goal. The generated outputs must precisely align with the
research goals, preferences and constraints defined by the scientist.
• Plausibility. The system outputs should be free of readily apparent flaws. Any potential contradictions
with prior literature or established knowledge must be explicitly stated and justified.
• Novelty. A key objective of the co-scientist system is to generate novel hypotheses, conjectures, and
research plans grounded in prior literature, rather than simply synthesizing existing information (a
capability already addressed by existing “deep research” tools [62]).
• Testability. The system outputs should be amenable to empirical validation within the constraints
specified by the scientist.
• Safety. The system outputs will be controlled to prevent enabling unsafe, unethical, or harmful research.
Aside from these default criteria, the co-scientist can be configured with additional criteria, preferences, and
constraints as needed. For instance, it can be configured to generate outputs in formats preferred by the
researcher to improve interpretability and readability.
Throughout this section, we employ a recurring example: generating hypotheses for exploring the biological
mechanisms of Amyotrophic Lateral Sclerosis (ALS) to illustrate the various components of the co-scientist
system. While this example has been reviewed by domain experts, it remains illustrative and may contain
errors. Importantly, this example does not aim to suggest potential therapeutic avenues for ALS and should
be interpreted with utmost caution. All the examples are listed in the Appendix Section A.3.
3.1 The AI co-scientist system overview
At a high level, the co-scientist system comprises four key components:
• Natural language interface. Scientists interact with and supervise the system primarily through
natural language. This allows them to not only define the initial research goal but also refine it at any
time, provide feedback on generated hypotheses (including their own solutions), and generally guide the
system’s progress.
• Asynchronous task framework. The co-scientist employs a multi-agent system where specialized
agents operate as worker processes within an asynchronous, continuous, and configurable task execution
framework. A dedicated Supervisor agent manages the worker task queue, assigns specialized agents to
these processes, and allocates resources. This design enables the system to flexibly and effectively utilize
computational resources and iteratively improve its scientific reasoning capabilities.
• Specialized agents. Following inductive biases and scientific priors derived from the scientific method,
the process of scientific reasoning and hypothesis generation is broken down into sub-tasks. Individual, 

specialized agents, each equipped with customized instruction prompts, are designed to execute these
sub-tasks. These agents operate as workers coordinated by the Supervisor agent.
• Context memory. In order to enable iterative computation and scientific reasoning over long time
horizons, the co-scientist uses a persistent context memory to store and retrieve states of the agents and
the system during the course of the computation.
The Gemini 2.0 model is the foundational LLM underpinning all agents in the co-scientist system. The specific
co-scientist design was arrived at with iterative developments and is reflective of the current capabilities of
the underlying LLMs.
3.2 From research goal to research plan configuration
The research goal, specified by the scientist, serves as the entry point to the co-scientist system. Leveraging the
multimodal and long context capabilities of Gemini 2.0 models, the co-scientist efficiently processes research
goals of varying complexity, from simple statements to extensive documents spanning tens of thousands of
natural language tokens or other relevant data (e.g., including hundreds of prior publication PDFs). The
research goal may also incorporate specific constraints, attributes, and preferences related to the scientist’s
particular laboratory setting or field of work.
The co-scientist system then parses the goal to derive a research plan configuration for generating research
proposals. This configuration captures the desired proposal preferences, attributes, and constraints. For
example, it specifies whether the co-scientist should exclusively propose novel hypotheses. It also specifies
the criteria for evaluating hypothesis quality, such as novelty and experimental feasibility. These criteria are
then used by the system during its auto-evaluation and improvement phases. The attributes, preferences, and
evaluation criteria can all be customized to a given research goal. To illustrate this process, we present an
example research goal and its corresponding parsed research plan configuration in Appendix Figure A.9, where
the goal is to develop a novel hypothesis related to phosphorylation of the Nuclear Pore Complex (NPC) as a
causative mechanism for ALS [63].
Based on the research plan configuration, the Supervisor agent initiates the creation of a task queue and begins
orchestrating the specialized agents. The system operates continuously and asynchronously. Periodically,
the Supervisor agent calculates a comprehensive set of summary statistics, reflecting the system’s state and
progress toward the specified research goal. These statistics inform decisions regarding resource allocation
and the determination of whether a terminal state for the overall computation has been reached. The state is
periodically written to the associated context memory of the system and leveraged as feedback in subsequent
rounds of computation. It also enables easy restarts in-case of any failure in the system components.
3.3 The specialized agents underpinning the AI co-scientist
At the core of the co-scientist system are a coalition of specialized agents, each orchestrated by the Supervisor
agent. These agents are designed to emulate the scientific reasoning process, enabling them to generate novel
hypotheses and research plans. They are also equipped to interact with external tools, such as web search
engines and specialized AI models, through application programming interfaces (APIs). These specialized
agents are enumerated below:
• Generation agent. The agent initiates the research process by generating the initial focus areas,
iteratively extending them and generating a set of initial hypotheses and proposals that address the
research goal. This involves exploring relevant literature using web search, synthesizing existing findings
into novel directions, and engaging in simulated scientific debates for iterative improvement.
• Reflection agent. This agent simulates the role of a scientific peer reviewer, critically examining the
correctness, quality, and novelty of the generated hypotheses and research proposals. Furthermore, it
evaluates the potential of each hypothesis to provide an improved explanation for existing research
observations (identified via literature search and review), particularly those that may be under explained.
• Ranking agent. An important abstraction in the co-scientist system is the notion of a tournament
where different research proposals are evaluated and ranked enabling iterative improvements. The
Ranking agent employs and orchestrates an Elo-based tournament [64] to assess and prioritize the 

generated hypotheses at any given time. This involves pairwise comparisons, facilitated by simulated
scientific debates, which allow for a nuanced evaluation of the relative merits of each proposal.
• Proximity agent. This agent asynchronously computes a proximity graph for generated hypotheses,
enabling clustering of similar ideas, de-duplication, and efficient exploration of the hypothesis landscape.
• Evolution agent. The co-scientist’s iterative improvement capability relies heavily on this agent, which
continuously refines the top-ranked hypotheses emerging from the tournament. Its refinement strategies
include synthesizing existing ideas, using analogies, leveraging literature for supporting details, exploring
unconventional reasoning, and simplifying concepts for clarity.
• Meta-review agent. This agent also enables the co-scientist’s continuous improvement by synthesizing
insights from all reviews, identifying recurring patterns in tournament debates, and using these findings
to optimize other agents’ performance in subsequent iterations. This also enhances the quality and
relevance of generated hypotheses and reviews in subsequent iterations. The agent also synthesizes
top-ranked hypotheses and reviews into a comprehensive research overview for review by the scientist.

The Supervisor agent’s seamless orchestration of these specialized agents enables the development of valid,
novel, and testable hypotheses and research plans tailored to the input research goal.
In summary, the Generation agent curates an initial list of research hypotheses satisfying a research goal.
These are then reviewed by the Reflection agent and evaluated in a tournament by the Ranking agent. The
Evolution, Proximity, and Meta-review agents operate on the tournament state to help improve the quality of
the system outputs.
The Supervisor agent periodically computes and writes to the context memory, a comprehensive suite of
statistics, including the number of hypotheses generated and requiring review, and the progress of the
tournament. These statistics also include analyses of the effectiveness of different hypothesis generation
methodologies (e.g., generating new ideas via the Generation agent vs. improving existing ideas via the
Evolution agent). Based on these statistics, the Supervisor agent then orchestrates subsequent system
operations, i.e., generating new hypotheses, reviews, tournaments, and improvements to existing hypotheses,
by strategically weighting and sampling the specialized agents for execution via the worker processes.
Importantly, the Meta-review agent enables feedback propagation and learning without back-propagation
techniques (e.g., fine-tuning or reinforcement learning) [65]. The Meta-review agent generates feedback 

applicable to all agents, which is simply appended to their prompts in the next iteration—a capability
facilitated by the long-context search and reasoning capabilities of the underlying Gemini 2.0 models. Through
this feedback loop, the co-scientist continuously learns and improves in subsequent iterations with more
compute scaling.
Finally, while our work leverages Gemini 2.0, the co-scientist framework is model-agnostic and portable to
other similar models or combinations thereof. Future LLM improvements will likely enhance the co-scientist’s
capabilities. The multi-agent architecture of the co-scientist is depicted and summarized in Figure 2.
We now describe the mechanisms of action of the specialized agents in more detail.
3.3.1 Generation agent
The co-scientist Generation agent employs a diverse array of techniques and tools to generate novel hypotheses,
such as the following:
• Literature exploration via web search. The agent iteratively searches the web, retrieves and reads
relevant research articles, and grounds its reasoning by summarizing prior work. It then builds on this
summary to generate novel hypotheses and research plans. An example prompt is given in Appendix
Figure A.1.
• Simulated scientific debates. Here, the Generation agent simulates scientific debates among experts
by employing self-critique and self-play techniques. These debates typically involve multiple turns of
conversations leading to a refined hypothesis generated at the end. An example prompt is given in
Appendix Figure A.2.
• Iterative assumptions identification. The agent iteratively identifies testable intermediate assumptions, which, if proven true, can lead to novel scientific discovery. These plausible assumptions and their
sub-assumptions are identified through conditional reasoning hops and subsequently aggregated into
complete hypotheses.
• Research expansion. To identify previously unexplored areas of the hypothesis space, the Generation
agent reviews existing hypotheses and the research overview and feedback provided by the Meta-review
agent in the previous iteration. This is used to inform additional exploration directions in the research
hypothesis space.
An example hypothesis and research proposal output from the Generation agent is presented in Appendix
Figure A.10 for the aforementioned research goal regarding explaining a basic mechanism related to ALS. The
Generation agent also summarizes and categorizes each generated hypothesis, allowing scientists to quickly
grasp the core ideas.
3.3.2 Reflection agent
Reviews are integral to the co-scientist’s effectiveness in generating novel proposals. The Reflection agent
searches relevant prior work (via web search or a dedicated scientist-provided repository), assesses existing
experimental evidence for or against a given hypothesis, and rigorously verifies the novelty, correctness, and
quality of generated outputs. Effective reviews filter inaccurate and, when stipulated, non-novel hypotheses.
Moreover, they also provide feedback to all other agents, driving continuous improvement. The Reflection
agent employs the following types of review:
• Initial review. Building on the co-scientist’s default evaluation criteria, the Reflection agent performs
an initial review assessing the correctness, quality, novelty, and a preliminary assessment of safety (ethics)
of the generated hypotheses. For a more in-depth discussion on safety considerations see Section 6. This
initial review, which doesn’t use external tools like web search, aims to quickly discard flawed, non-novel,
or otherwise unsuitable hypotheses.
• Full review. If a hypothesis passes the initial review, the Reflection agent performs a full review,
leveraging external tools and web searches to identify relevant articles for improved reasoning and
grounding. This review evaluates the hypothesis’s correctness, quality, and novelty similar to the initial
review but with full literature search. For correctness and quality, the agent scrutinizes underlying 

assumptions and reasoning. For novelty, it summarizes known aspects of the hypothesis and then
judges their novelty based on existing literature. An example full novelty review is shown in Appendix
Figure A.11, and an example of review critiques is in Appendix Figure A.12. A complete full review
example is shown in Appendix Figure A.13.
• Deep verification review. The Reflection agent also conducts a deep verification review, decomposing
the hypothesis into constituent assumptions. Each assumption is further broken down into fundamental
sub-assumptions, decontextualized, and independently evaluated for correctness to identify invalidating
elements for subsequent filtering. Concurrently, the reasons for potential hypothesis invalidation due
to incorrect assumptions are summarized. This deep verification helps the co-scientist detect subtle
errors within complex hypotheses, such as flaws in reasoning or inaccurate experimental protocols. An
identified error doesn’t necessarily invalidate the core hypothesis; the Reflection agent assesses whether
the incorrect assumption is fundamental to the hypothesis and incorporates this reasoning into the
review. Non-fundamental errors can be addressed during subsequent refinement stages. An example deep
verification review is provided in Appendix Figure A.14 for the previously introduced ALS hypothesis.
We also show another example of a deep verification review via probing questions in the context of drug
repurposing for AML in Appendix Figure A.15.
• Observation review. In addition, the Reflection agent also explores whether a given hypothesis can
account for long-tail observations from prior experimental results. This review aims to determine if
the hypothesis can provide insights on existing experimental findings and observed phenomena within
relevant articles. For each observation, the agent assesses if the hypothesis is a superior explanation
over existing ones, assuming its validity. Positive observations are summarized and appended to the
hypothesis. Note that this review often completes without any important findings (as in the case of
the ALS hypothesis example). An example prompt to generate observations is provided in Appendix
Figure A.3. An illustrative example of an observation review is provided in Appendix Figure A.16 in
the context of an alternate hypothesis for explaining a mechanism of anti-microbial resistance.
• Simulation review. The Reflection agent also reviews hypotheses by simulating them in a step-wise
fashion (e.g., simulating the mechanism of action or the proposed experiment in the proposal). This
simulation allows the agent to identify and summarize potential failure scenarios. This review method is
built on the assumption that frontier LLMs may have developed an internal world model that enables
them to simulate and accurately predict various scientific phenomena.
• Recurrent/tournament review. The Reflection agent adapts its full reviews based on the co-scientist’s
growing knowledge. By analyzing reviewed hypotheses and results of the tournament conducted by the
Ranking agent, the Reflection agent identifies recurring issues and improvement opportunities, refining
its reviews accordingly.
Additionally, the co-scientist can incorporate reviews from expert scientists to guide ranking and improvements
(further discussed in Section 3.4). We aim to have the Reflection agent’s comprehensive set of reviews cover
the common methods scientists employ when critiquing and refining research hypotheses and proposals.
3.3.3 Ranking agent
The AI co-scientist explores numerous hypotheses and research proposals towards a research goal, necessitating
a ranking mechanism to prioritize computational resources toward the most promising candidates. This task
is performed by the Ranking agent. The agent uses an Elo-based tournament [64] to automatically evaluate
and rank all hypotheses, providing supporting rationale. This ranking serves to communicate to scientists
an ordered list of research hypotheses and proposals aligned with the research goal. Despite its assumptions
and limitations [66], Elo remains a good proxy for relative ranking, and it has previously been applied to
rank extracted patterns and ideas in games [67]. In the future, extensions may be considered [68]. We set the
initial Elo rating of 1200 for the newly added hypothesis.
Because the tournament is computationally intensive, the Ranking agent employs several optimization
strategies. Top-ranked hypotheses are compared pairwise in tournament matches through multi-turn scientific
debates [69]. This mitigates ordering bias and focuses on novelty, correctness, and testability. Lower-ranked
hypotheses undergo single-turn comparisons in a pairwise fashion in their tournament match. The agent
concludes each comparison with a decision regarding which hypothesis is better. Appendix Figure A.4 and 

Appendix Figure A.5 show example prompts. Appendix Figure A.17 shows an example of the Ranking agent
conducting a scientific debate match in a tournament to compare two hypotheses.
The Ranking agent prioritizes tournament matches as follows: (1) hypotheses are more likely to be compared
with similar ones (based on the Proximity agent’s graph, described in the next section); (2) newer and
top-ranking hypotheses are prioritized for participation in tournament matches. Successful hypotheses quickly
achieve favorable rankings and this informs the tournament state for subsequent iterations.
3.3.4 Proximity agent
The Proximity agent calculates the similarity between research hypotheses and proposals, and builds a
proximity graph, taking into account the specific research goal. Although it doesn’t directly participate in
hypothesis generation, the Proximity agent assists the Ranking agent in organizing tournament matches and
showcasing a diverse range of ideas related to the research goal. This allows scientists to quickly explore areas
of interest and easily identify related concepts.
3.3.5 Evolution agent
The Evolution agent continuously refines and improves existing hypotheses and proposals using several
approaches including:
• Enhancement through grounding. Here the agent attempts to improve hypotheses by identifying
weaknesses, generating search queries, retrieving and reading articles, suggesting improvements and
elaborating on details to fill reasoning gaps.
• Coherence, practicality and feasibility improvements. The agent aims to address issues and
creates more coherent hypotheses, potentially rectifying underlying problems with invalid initial assumptions. The agent also refines the hypotheses to make them more practical and feasible. Appendix
Figure A.6 provides an example of the feasibility improvement prompt.
• Inspiration from existing hypotheses. The agent additionally creates new hypotheses inspired by
single or multiple top-ranked hypotheses.
• Combination. The agent also attempts to directly combine the best aspects of several top-ranking
hypotheses to create new hypotheses.
• Simplification. The agent simplifies hypotheses for easier verification and testing.
• Out-of-box thinking. The agent also explores out-of-the-box ideas by moving away from a subset of
hypotheses and generating divergent ones. Appendix Figure A.7 provides an example prompt for this.
The Evolution agent generates new hypotheses; it doesn’t modify or replace existing ones. This strategy
protects the quality of top-ranked hypotheses from flawed improvements, as each new hypothesis must also
compete in the tournament. The evolution of research hypotheses and proposals also allows the co-scientist to
iteratively combine different improvement techniques and gradually improve the quality of the results.
3.3.6 Meta-review agent
The Meta-review agent plays a crucial role in the co-scientist’s feedback loop, enabling self-improvement in
scientific reasoning. This agent operates on the tournament state and summarizes common patterns identified
in reviews and scientific debates in the tournament matches into a meta-review critique.
By synthesizing insights from all reviews, the meta-review provides valuable feedback to the Reflection agent,
leading to more thorough and reliable future reviews. This helps prevent oversight of critical details. Consider
the illustrative example of a identifying a repurposing drug candidate for ALS as a research goal: while only
90% of individual reviews might correctly identify a blood-brain barrier permeability issue in a proposed
candidate, the meta-review ensures that all future reviews by the Reflection Agent definitively address this
crucial factor. Hypothesis and research proposal generation is also enhanced by the meta-review’s identification
of recurring issues. While the Generation agent uses this feedback selectively to avoid over fitting to these
review critiques, it helps prevent the recurrence of common issues.
Appendix Figure A.8 provides an example prompt for the meta-review. In Appendix Figure A.18-A.19, we
showcase an example of the summarized meta-review critique generated for the reviews of the previously 

introduced ALS mechanism hypotheses.
Research overview generation. The Meta-review agent periodically synthesizes top-ranked hypotheses into
a research overview, providing a roadmap for future research. This overview outlines potential research areas
and directions relevant to the research goal, justifying their importance and suggesting specific experiments
within each. Each area includes illustrative example topics. The research overview also serves as an additional
input to the Generation agent in subsequent iterations.
The research overview serves to effectively map the boundary of current knowledge relevant to the research
goal in the co-scientist system and helps highlight future areas of exploration. In Appendix Figure A.20-A.21,
we show an example of a research overview for the ALS mechanism research goal.
The Meta-review agent can further format these overviews using constrained decoding techniques [70] to
adhere to common research publication and grant formats (e.g., National Institute of Health (NIH) Specific
Aims Page format). We demonstrate the effectiveness of this in subsequent sections.
Research contacts identification. The Meta-review agent uses prior literature review to suggest qualified
domain experts for research hypotheses and proposal review, including the reasoning behind each suggestion.
These potential contacts are summarized in the research overview, providing researchers with additional
perspectives and potential avenues for collaborations. An example research contact (with the researcher name
redacted) is shown in Appendix Figure A.22.
3.4 Expert-in-the-loop interactions with the co-scientist
The AI co-scientist empowers scientists to actively guide the system through an expert-in-the-loop design
(Figure 2). Scientists can interact with the system in several ways:
• Refine the initial research goal in light of the generated hypotheses and research overview.
• Provide manual reviews of generated hypotheses (see Section 3.3.2 for other system generated review
types), which the co-scientist uses to evaluate and improve the hypotheses and proposals.
• Contribute their own hypotheses and proposals for inclusion in the tournament, where they are ranked
alongside and can be combined with system-generated hypotheses and proposals.
• Direct the co-scientist to follow up on specific research directions (for example restricted to a smaller
collection of prior publications). When this research is referenced in the research goal, the co-scientist
can prioritize generation methods that can access and synthesize it.
3.5 Tool use in AI co-scientist
The co-scientist leverages various tools during the generation, review, and improvement of hypotheses and
research proposals. Web search and retrieval are primary tools, important for grounded, up-to-date hypotheses.
For research goals that explore a constrained space of possibilities (e.g., all known cell receptors of a specific
type or all FDA-approved drugs), the co-scientist agents utilize domain-specific tools, such as open databases,
to constrain searches and generate hypotheses. The co-scientist can also index and search a private repository
of publications specified by the scientist.
Finally, the system can utilize and incorporate feedback from specialized AI models like AlphaFold. We
demonstrate this qualitatively with a protein design example in the Appendix Section A.6.

Figure 2 | The AI co-scientist multi-agent architecture design. The co-scientist accepts a natural language research goal
from the user and parses this into a research plan configuration. This plan is then dispatched to the Supervisor agent which
evaluates this plan to assigns weights and resources to each specialized agent and subsequently queues them as worker processes
in a task queue according to these weights. The worker processes execute the queue of agent actions, and the system ultimately
aggregates all information to formulate a research overview with detailed hypotheses and proposals for the scientist. The red
boxes in the “The AI co-scientist specialized agents” section denote individual agents each with their own unique logic and role.
The blue boxes indicate the scientist-in-the-loop inputs and feedback. The dark gray arrows represent the information flow
through the co-scientist system, while the red arrows represent the information feedback loop between the specialized agents.


---
YOUR GOAL IS TO CREATE SUCH A SYSTEM FROM SCRATCH.

We should:
- FOLLOW BEST CODING PRACTICES (well structured, good, modular architecture, reusable things, not too abstract, so on).
- Develop it an iterative way (from simpler to more complex). 
- After each "iteration", you should RUN THE SYSTEM FROM SCRATCH to make sure it works correctly.
- After testing it, we should commit in git the changes. You tell me and I will do it manually (and also analyze the changes).

When you add some files for testing or utilities and so on, do it INSIDE particular folders. Do it in an organized way following best practices, not all in the root.

VERY IMPORTANT: YOU HAVE TO USE THE CONDA ENV: “conda activate co_scientist”

Remember, remove all the unnecessary files and folders in the repo. But if you change big things, we should test the system to see that everything is ok.

FOLLOW BEST PRACTICES but always test that everything is working after major changes!

The models should be run using an AZURE OPENAI service as the default provider. Also include OPENAI, ollama and others as fallbacks. It should be configurable.
I already have a .env file with the credentials for all of them.